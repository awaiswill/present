{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/present/blob/master/youtube/gpu/keras-dual-gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jeff Heaton\n",
    "\n",
    "You can follow me at any of:\n",
    "\n",
    "* [YouTube](https://www.youtube.com/user/HeatonResearch)\n",
    "* [Website](https://www.heatonresearch.com/)\n",
    "* [GitHub](https://github.com/jeffheaton)\n",
    "* [Twitter](https://twitter.com/jeffheaton)\n",
    "\n",
    "\n",
    "## Multi-GPU Support\n",
    "\n",
    "Keras makes it easy to use more than one GPU for neural network training or scoring. This tutorial shows how to train a model for the [Cats vs Dogs](https://www.kaggle.com/c/dogs-vs-cats) dataset. Not all models will necessarily benefit from multiple GPUs.  Generally larger batch sizes and more complex neural networks benefit from multiple GPUs.\n",
    "\n",
    "The technique presented in this notebook can train with between 1 and 8 GPUs on a single host.  It is also possable to train larger numbers of GPUs on multiple hosts; however, a slightly different approach is needed. \n",
    "\n",
    "First, we will list what GPUs are available on the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " /device:CPU:0 || Unnamed device || CPU || 256.0 MiB\n",
      " /device:GPU:0 ||  RTX A6000 || GPU || 44.0 GiB\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "devices = device_lib.list_local_devices()\n",
    "\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "for d in devices:\n",
    "    t = d.device_type\n",
    "    name = d.physical_device_desc\n",
    "    l = [item.split(':',1) for item in name.split(\", \")]\n",
    "    name_attr = dict([x for x in l if len(x)==2])\n",
    "    dev = name_attr.get('name', 'Unnamed device')\n",
    "    print(f\" {d.name} || {dev} || {t} || {sizeof_fmt(d.memory_limit)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain Cats vs Dogs Dataset\n",
    "\n",
    "First, we obain the Cats vs Dogs dataset.  We use the [tensorflow_datasets](https://www.tensorflow.org/datasets) library access this data. Any data can be used, **tensorflow_datasets** makes loading common datasets a simple process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Some requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /job:localhost/replica:0/task:0/device:GPU:0,/job:localhost/replica:0/task:0/device:GPU:1\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Number of devices: 2\n",
      "\u001b[1mDownloading and preparing dataset 786.68 MiB (download: 786.68 MiB, generated: Unknown size, total: 786.68 MiB) to /home/jeff/tensorflow_datasets/cats_vs_dogs/4.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4a03534be6446292fa4c44baacc9f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb37bd927154bdf99f1bde4dc39a2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/23262 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:1738 images were corrupted and were skipped\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling cats_vs_dogs-train.tfrecord...:   0%|          | 0/23262 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset cats_vs_dogs downloaded and prepared to /home/jeff/tensorflow_datasets/cats_vs_dogs/4.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "GPUS = [\"GPU:0\", \"GPU:1\"]\n",
    "\n",
    "def process(image, label):\n",
    "    image = tf.image.resize(image, [299, 299]) / 255.0\n",
    "    return image, label\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy( GPUS )\n",
    "print('Number of devices: %d' % strategy.num_replicas_in_sync) \n",
    "\n",
    "batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "\n",
    "dataset = tfds.load(\"cats_vs_dogs\", split=tfds.Split.TRAIN, as_supervised=True)\n",
    "dataset = dataset.map(process).shuffle(500).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Distributed Training\n",
    "\n",
    "Training with multiple GPUs is not much different than training with a single GPU.  Wrapping the model creation and compilation with a mirror strategy scope is all that is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "364/364 [==============================] - 272s 573ms/step - loss: 0.6940 - sparse_categorical_accuracy: 0.6598\n",
      "Epoch 2/5\n",
      "364/364 [==============================] - 209s 573ms/step - loss: 0.4446 - sparse_categorical_accuracy: 0.7963\n",
      "Epoch 3/5\n",
      "364/364 [==============================] - 209s 572ms/step - loss: 0.2588 - sparse_categorical_accuracy: 0.8929\n",
      "Epoch 4/5\n",
      "364/364 [==============================] - 208s 571ms/step - loss: 0.1797 - sparse_categorical_accuracy: 0.9261\n",
      "Epoch 5/5\n",
      "364/364 [==============================] - 209s 572ms/step - loss: 0.1386 - sparse_categorical_accuracy: 0.9453\n",
      "Training time: 0:18:30.76\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import time\n",
    "\n",
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "EPOCHS = 5\n",
    "LR = 0.001 \n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "start = time.time()\n",
    "with strategy.scope():\n",
    "    model = tf.keras.applications.InceptionResNetV2(weights=None, classes=2)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "        loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "        metrics=[tf.keras.metrics.sparse_categorical_accuracy]\n",
    "    )\n",
    "\n",
    "model.fit(dataset, epochs=EPOCHS)\n",
    "\n",
    "elapsed = time.time()-start\n",
    "print (f'Training time: {hms_string(elapsed)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 15:54.81 - Dual Quadro RTX 8000\n",
    "* 18:30.76 - Single RTX A6000\n",
    "* 24:17.07 - Single TITAN RTX\n",
    "* 24:48.10 - Single Tesla V100-SXM2-16GB\n",
    "* 26:23.19 - Single Quadro RTX 8000\n",
    "* 37:48.45 - Single Tesla P100-PCIE-16GB\n",
    "* 50:36.50 - Single Quadro RTX 5000\n",
    "* 1:10:08.54 - Single Tesla T4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1/5\n",
    "364/364 [==============================] - 453s 1s/step - loss: 0.5174 - sparse_categorical_accuracy: 0.7493\n",
    "Epoch 2/5\n",
    "364/364 [==============================] - 301s 826ms/step - loss: 0.2556 - sparse_categorical_accuracy: 0.8929\n",
    "Epoch 3/5\n",
    "364/364 [==============================] - 309s 848ms/step - loss: 0.1792 - sparse_categorical_accuracy: 0.9267\n",
    "Epoch 4/5\n",
    "364/364 [==============================] - 306s 842ms/step - loss: 0.1344 - sparse_categorical_accuracy: 0.9452\n",
    "Epoch 5/5\n",
    "364/364 [==============================] - 307s 844ms/step - loss: 0.1104 - sparse_categorical_accuracy: 0.9571\n",
    "Training time: 0:30:46.31\n",
    "\n",
    "8000\n",
    "Epoch 1/5\n",
    "364/364 [==============================] - 178s 489ms/step - sparse_categorical_accuracy: 0.7405 - loss: 0.5237\n",
    "Epoch 2/5\n",
    "364/364 [==============================] - 180s 494ms/step - sparse_categorical_accuracy: 0.8725 - loss: 0.2928\n",
    "Epoch 3/5\n",
    "364/364 [==============================] - 179s 492ms/step - sparse_categorical_accuracy: 0.9205 - loss: 0.1900\n",
    "Epoch 4/5\n",
    "364/364 [==============================] - 179s 491ms/step - sparse_categorical_accuracy: 0.9402 - loss: 0.1445\n",
    "Epoch 5/5\n",
    "364/364 [==============================] - 178s 490ms/step - sparse_categorical_accuracy: 0.9536 - loss: 0.1136\n",
    "Training time: 0:16:14.80\n",
    "\n",
    "Epoch 1/5\n",
    "727/727 [==============================] - 310s 427ms/step - loss: 0.5647 - sparse_categorical_accuracy: 0.7119\n",
    "Epoch 2/5\n",
    "727/727 [==============================] - 308s 424ms/step - loss: 0.3291 - sparse_categorical_accuracy: 0.8558\n",
    "Epoch 3/5\n",
    "727/727 [==============================] - 307s 422ms/step - loss: 0.2029 - sparse_categorical_accuracy: 0.9166\n",
    "Epoch 4/5\n",
    "727/727 [==============================] - 306s 421ms/step - loss: 0.1567 - sparse_categorical_accuracy: 0.9347\n",
    "Epoch 5/5\n",
    "727/727 [==============================] - 306s 421ms/step - loss: 0.1300 - sparse_categorical_accuracy: 0.9469\n",
    "Training time: 0:26:23.19"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tf-latest)",
   "language": "python",
   "name": "tf-latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
