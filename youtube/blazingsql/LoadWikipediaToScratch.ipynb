{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "WIKIPEDIA_ROOT = \"/media/jeff/Data/data/wikipedia2\"\n",
    "WIKIPEDIA_DL = os.path.join(WIKIPEDIA_ROOT, 'dl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://dumps.wikimedia.org/enwiki/20210320\n",
      "https://dumps.wikimedia.org/enwiki/20210320/enwiki-20210320-pages-meta-current1.xml-p1p41242.bz2\n",
      "https://dumps.wikimedia.org/enwiki/20210320/enwiki-20210320-pages-meta-current2.xml-p41243p151573.bz2\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "import urllib\n",
    "import posixpath\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "WIKIPEDIA_URL = 'https://dumps.wikimedia.org/'\n",
    "WIKIPEDIA_LANG = 'enwiki'\n",
    "\n",
    "def get_wikidump_available(wiki_url=WIKIPEDIA_URL,wiki_lang=WIKIPEDIA_LANG):\n",
    "    base_url = posixpath.join(wiki_url, wiki_lang)\n",
    "    index = urllib.request.urlopen(base_url).read()\n",
    "    soup_index = BeautifulSoup(index, 'html.parser')\n",
    "    # Find the links on the page\n",
    "    return [a['href'] for a in soup_index.find_all('a') if \n",
    "             a.has_attr('href')]\n",
    "\n",
    "def get_latest_wikidump(dumps):\n",
    "    lst = []\n",
    "    for dump in dumps:\n",
    "        try:\n",
    "            idx = dump.index('/')\n",
    "            \n",
    "            if idx != -1:\n",
    "                dump = dump[:idx]\n",
    "            \n",
    "            lst.append(int(dump))\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "    return max(lst)\n",
    "\n",
    "def wikidump_download(wiki_url=WIKIPEDIA_URL,wiki_lang=WIKIPEDIA_LANG, timestamp=None):\n",
    "    if not timestamp:\n",
    "        timestamp = get_latest_wikidump(get_wikidump_available(wiki_url,wiki_lang))\n",
    "    \n",
    "    dump_url = posixpath.join(wiki_url, wiki_lang, str(timestamp))\n",
    "    print(dump_url)\n",
    "    dump_html = urllib.request.urlopen(dump_url).read()\n",
    "    soup_dump = BeautifulSoup(dump_html, 'html.parser')\n",
    "    files = soup_dump.find_all('li', {'class': 'file'})\n",
    "    \n",
    "    for items in soup_dump.find_all('li', {'class': 'file'}):\n",
    "        for child in items.children:\n",
    "            if isinstance(child,bs4.element.Tag):\n",
    "                f = child.get('href', default=None)\n",
    "                if f and '-stub' not in f and '-current' in f and 'xml-' in f:\n",
    "                    url = urllib.parse.urljoin(WIKIPEDIA_URL, f)\n",
    "                    a = urlparse(url)\n",
    "                    target_file = os.path.join(WIKIPEDIA_DL,os.path.basename(a.path))\n",
    "                    print(url)\n",
    "                    urllib.request.urlretrieve(url, target_file)\n",
    "    \n",
    "\n",
    "wikidump_download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose the Dump to Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_url = posixpath.join(base_url, 'latest')\n",
    "\n",
    "print(dump_url)\n",
    "# Retrieve the html\n",
    "dump_html = urllib.request.urlopen(dump_url).read()\n",
    "# Convert to a soup\n",
    "soup_dump = BeautifulSoup(dump_html, 'html.parser')\n",
    "# Find list elements with the class file\n",
    "files = soup_dump.find_all('li', {'class': 'file'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for items in soup_dump.find_all('li', {'class': 'file'}):\n",
    "    for child in items.children:\n",
    "        if isinstance(child,bs4.element.Tag):\n",
    "            f = child.get('href', default=None)\n",
    "            if f and '-stub' not in f and '-current' in f and 'xml-' in f:\n",
    "                url = urllib.parse.urljoin(WIKIPEDIA_URL, f)\n",
    "                a = urlparse(url)\n",
    "                target_file = os.path.join(WIKIPEDIA_DL,os.path.basename(a.path))\n",
    "                print(url)\n",
    "                urllib.request.urlretrieve(url, target_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
