{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "amended-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_wikipedia import *\n",
    "\n",
    "WIKIPEDIA_ROOT = \"/home/jeff/data/wikipedia2\"\n",
    "WIKIPEDIA_DL = os.path.join(WIKIPEDIA_ROOT, 'dl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-portrait",
   "metadata": {},
   "source": [
    "# Download Wikipedia Dump Data\n",
    "\n",
    "This part only needs to be run once, at the beginning to download Wikipedia to somewhere on your system.  Note, this part will take a long time!  It will transfer much data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "recreational-saver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://dumps.wikimedia.org/enwiki/20210320\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bs4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f66fccbce6da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwikidump_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWIKIPEDIA_DL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/projects/present/youtube/blazingsql/process_wikipedia.py\u001b[0m in \u001b[0;36mwikidump_download\u001b[0;34m(target_path, wiki_url, wiki_lang, timestamp)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msoup_dump\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'li'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbs4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'-stub'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'-current'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'xml-'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bs4' is not defined"
     ]
    }
   ],
   "source": [
    "wikidump_download(WIKIPEDIA_DL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-picture",
   "metadata": {},
   "source": [
    "# Process Wikipedia Data\n",
    "\n",
    "This can be run after the previous section has downloaded Wikipedia data.  This example creates 3 files from extracted Wikipedia data:\n",
    "\n",
    "* article.csv - A listing of all articles on Wikipedia, with their Wikipedia ID.\n",
    "* redirect.csv - A listing of all redirects of articles on Wikipedia. e.g. USA to United_States\n",
    "* template.csv - A listing of all templates on Wikipedia.  These are the \"types\" of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessPagesWorker():\n",
    "    def __init__(self, config, outputQueue):\n",
    "        self.config = config\n",
    "        self.outputQueue = outputQueue\n",
    "        \n",
    "    def process_template(self, id, title):\n",
    "        self.outputQueue.put(\n",
    "            {'template': [id, title] }\n",
    "        )\n",
    "    \n",
    "    def process_article(self, id, title):\n",
    "        self.outputQueue.put(\n",
    "            {'article': [id, title] }\n",
    "        )\n",
    "    \n",
    "    def process_redirect(self, id, title, redirect):\n",
    "        self.outputQueue.put(\n",
    "            {'redirect': [id, title, redirect] }\n",
    "        )\n",
    "        \n",
    "    def report_progress(self, completed):\n",
    "        self.outputQueue.put({\"completed\": completed})\n",
    "        \n",
    "    def close(self):\n",
    "        self.articles_fp.close()\n",
    "        self.redirect_fp.close()\n",
    "        self.template_fp.close()\n",
    "\n",
    "class ProcessPages:\n",
    "    def __init__(self, output_path):        \n",
    "        pathArticles = os.path.join(output_path, \"article.csv\")\n",
    "        pathRedirect = os.path.join(output_path, \"redirect.csv\")\n",
    "        pathTemplate = os.path.join(output_path, \"template.csv\")\n",
    "        \n",
    "        self.articles_fp = codecs.open(pathArticles, \"w\", ENCODING)\n",
    "        self.redirect_fp = codecs.open(pathRedirect, \"w\", ENCODING)\n",
    "        self.template_fp = codecs.open(pathTemplate, \"w\", ENCODING)\n",
    "    \n",
    "        self.articlesWriter = csv.writer(self.articles_fp, quoting=csv.QUOTE_MINIMAL)\n",
    "        self.redirectWriter = csv.writer(self.redirect_fp, quoting=csv.QUOTE_MINIMAL)\n",
    "        self.templateWriter = csv.writer(self.template_fp, quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        self.articlesWriter.writerow(['id', 'title'])\n",
    "        self.redirectWriter.writerow(['id', 'title', 'redirect'])\n",
    "        self.templateWriter.writerow(['id', 'title'])\n",
    "    \n",
    "    def handle_event(self, evt):\n",
    "\n",
    "        if \"article\" in evt:\n",
    "            self.articlesWriter.writerow(evt['article'])\n",
    "        elif \"template\" in evt:\n",
    "            self.templateWriter.writerow(evt['template'])\n",
    "        elif \"redirect\" in evt:\n",
    "            self.redirectWriter.writerow(evt['redirect'])\n",
    "            \n",
    "    def get_worker_class(self, outputQueue, config):\n",
    "        return ProcessPagesWorker(config, outputQueue)\n",
    "    \n",
    "\n",
    "wiki = ExtractWikipedia(\n",
    "    ProcessPages(WIKIPEDIA_ROOT), # where you want the extracted Wikipedia files to go\n",
    "    WIKIPEDIA_DL #Location you downloaded Wikipedia to\n",
    ")\n",
    "wiki.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-norfolk",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
